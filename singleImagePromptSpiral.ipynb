{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stepbot/StableDiffusionNotebooks/blob/main/singleImagePromptSpiral.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zHkHsdtnry57"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aIrgth7sqFML"
      },
      "outputs": [],
      "source": [
        "!pip install diffusers[torch]==0.11.1 transformers scipy ftfy accelerate open_clip_torch pip install bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_id = \"dreamlike-art/dreamlike-photoreal-2.0\""
      ],
      "metadata": {
        "id": "FqqIFzJPZBh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xSKWBKFPArKS"
      },
      "outputs": [],
      "source": [
        "from diffusers import StableDiffusionPipeline\n",
        "from transformers import pipeline\n",
        "import torch\n",
        "from transformers import AutoProcessor, AutoTokenizer, AutoImageProcessor, AutoModelForCausalLM, BlipForConditionalGeneration, Blip2ForConditionalGeneration, VisionEncoderDecoderModel\n",
        "import open_clip\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "pipe = StableDiffusionPipeline.from_pretrained(model_id)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# git_processor_base = AutoProcessor.from_pretrained(\"microsoft/git-base-coco\")\n",
        "# git_model_base = AutoModelForCausalLM.from_pretrained(\"microsoft/git-base-coco\")\n",
        "\n",
        "git_processor_large_coco = AutoProcessor.from_pretrained(\"microsoft/git-large-coco\")\n",
        "git_model_large_coco = AutoModelForCausalLM.from_pretrained(\"microsoft/git-large-coco\")\n",
        "\n",
        "git_processor_large_textcaps = AutoProcessor.from_pretrained(\"microsoft/git-large-r-textcaps\")\n",
        "git_model_large_textcaps = AutoModelForCausalLM.from_pretrained(\"microsoft/git-large-r-textcaps\")\n",
        "\n",
        "# blip_processor_base = AutoProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "# blip_model_base = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "\n",
        "blip_processor_large = AutoProcessor.from_pretrained(\"Salesforce/blip-image-captioning-large\")\n",
        "blip_model_large = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-large\")\n",
        "\n",
        "# blip2_processor = AutoProcessor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n",
        "# blip2_model = Blip2ForConditionalGeneration.from_pretrained(\"Salesforce/blip2-opt-2.7b\", torch_dtype=torch.float16)\n",
        "\n",
        "blip2_processor_8_bit = AutoProcessor.from_pretrained(\"Salesforce/blip2-opt-6.7b\")\n",
        "blip2_model_8_bit = Blip2ForConditionalGeneration.from_pretrained(\"Salesforce/blip2-opt-6.7b\", device_map=\"auto\", load_in_8bit=True)\n",
        "\n",
        "# vitgpt_processor = AutoImageProcessor.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
        "# vitgpt_model = VisionEncoderDecoderModel.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
        "# vitgpt_tokenizer = AutoTokenizer.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
        "\n",
        "coca_model, _, coca_transform = open_clip.create_model_and_transforms(\n",
        "  model_name=\"coca_ViT-L-14\",\n",
        "  pretrained=\"mscoco_finetuned_laion2B-s13B-b90k\"\n",
        ")\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# git_model_base.to(device)\n",
        "# blip_model_base.to(device)\n",
        "git_model_large_coco.to(device)\n",
        "git_model_large_textcaps.to(device)\n",
        "blip_model_large.to(device)\n",
        "# vitgpt_model.to(device)\n",
        "coca_model.to(device)\n",
        "# blip2_model.to(device)\n",
        "\n",
        "def generate_caption(processor, model, image, tokenizer=None, use_float_16=False):\n",
        "    inputs = processor(images=image, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    if use_float_16:\n",
        "        inputs = inputs.to(torch.float16)\n",
        "    \n",
        "    generated_ids = model.generate(pixel_values=inputs.pixel_values, max_length=50)\n",
        "\n",
        "    if tokenizer is not None:\n",
        "        generated_caption = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "    else:\n",
        "        generated_caption = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "   \n",
        "    return generated_caption\n",
        "\n",
        "\n",
        "def generate_caption_coca(model, transform, image):\n",
        "    im = transform(image).unsqueeze(0).to(device)\n",
        "    with torch.no_grad(), torch.cuda.amp.autocast():\n",
        "        generated = model.generate(im, seq_len=20)\n",
        "    return open_clip.decode(generated[0].detach()).split(\"<end_of_text>\")[0].replace(\"<start_of_text>\", \"\")\n",
        "\n",
        "\n",
        "def generate_captions(image):\n",
        "    # caption_git_base = generate_caption(git_processor_base, git_model_base, image)\n",
        "\n",
        "    caption_git_large_coco = generate_caption(git_processor_large_coco, git_model_large_coco, image)\n",
        "\n",
        "    caption_git_large_textcaps = generate_caption(git_processor_large_textcaps, git_model_large_textcaps, image)\n",
        "\n",
        "    # caption_blip_base = generate_caption(blip_processor_base, blip_model_base, image)\n",
        "\n",
        "    caption_blip_large = generate_caption(blip_processor_large, blip_model_large, image)\n",
        "\n",
        "    # caption_vitgpt = generate_caption(vitgpt_processor, vitgpt_model, image, vitgpt_tokenizer)\n",
        "\n",
        "    caption_coca = generate_caption_coca(coca_model, coca_transform, image)\n",
        "\n",
        "    # caption_blip2 = generate_caption(blip2_processor, blip2_model, image, use_float_16=True).strip()\n",
        "\n",
        "    caption_blip2_8_bit = generate_caption(blip2_processor_8_bit, blip2_model_8_bit, image, use_float_16=True).strip()\n",
        "\n",
        "    return caption_git_large_coco, caption_git_large_textcaps, caption_blip_large, caption_coca, caption_blip2_8_bit"
      ],
      "metadata": {
        "id": "OgIQ-lwY7_2C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")"
      ],
      "metadata": {
        "id": "8e9kxDw3LNtd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "\n",
        "pipe.safety_checker=None\n",
        "pipe = pipe.to(\"cuda\")\n",
        "\n",
        "\n",
        "#generator = torch.Generator(\"cuda\").manual_seed(0)\n",
        "original_prompt = \"emma, a beautiful woman\"\n",
        "prompt = original_prompt\n",
        "n_prompt = \"worst quality, bad anatomy,messed up face, low resolution, extra limbs, unnatural, messed up body, extra fingers\"\n",
        "\n",
        "for ii in range(10):\n",
        "  image = pipe(prompt=prompt, negative_prompt=n_prompt, guidance_scale=7.5, num_inference_steps=30, height=768, width=768).images[0]\n",
        "  image.save(\"output\"+str(ii)+\".png\")\n",
        "  image\n",
        "  captions = generate_captions(image)\n",
        "  prompt = original_prompt\n",
        "  prompt+=\", \"\n",
        "  for caption in captions:\n",
        "    prompt+=caption\n",
        "    prompt+=\", \"\n",
        "  words = prompt.split()\n",
        "  prompt = summarizer(prompt, max_length=int(len(words)/2), min_length=int(len(words)/len(captions)), do_sample=False)[0]['summary_text']\n",
        "  \n",
        "  # show image\n",
        "  plt.figure(figsize = (8,8))\n",
        "  plt.imshow(image)\n",
        "  plt.axis(\"off\")\n",
        "  plt.title(prompt)\n",
        "  plt.show()\n",
        "\n",
        "  \n",
        "\n"
      ],
      "metadata": {
        "id": "mH3hO-DFZxEC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}